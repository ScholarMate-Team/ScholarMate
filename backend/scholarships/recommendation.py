import openai
import json
import re
from datetime import datetime
from django.db.models import QuerySet, Q, Case, When, Value
from django.conf import settings
from scholarships.models import Scholarship
from userinfor.models import UserScholarship
from django.db import models
from typing import List, Dict # íƒ€ì… íŒíŠ¸ ì¶”ê°€

# --- API í‚¤ ì„¤ì • ---
openai.api_key = settings.OPENAI_API_KEY

# (GPT ìƒí˜¸ì‘ìš© í—¬í¼ í•¨ìˆ˜ë“¤ì€ ë³€ê²½ ì—†ìŒ)
def call_gpt(prompt: str) -> str:
    """OpenAI GPT ëª¨ë¸ì„ í˜¸ì¶œí•˜ê³  ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # ğŸš¨ ì‹¤í–‰ ì „ openai.api_key = settings.OPENAI_API_KEY ì„¤ì •ì´ í•„ìˆ˜
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì¥í•™ê¸ˆ ì¶”ì²œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ ì •í™•í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1
        )
        gpt_response_content = response['choices'][0]['message']['content']
        print("DEBUG: [GPT ì‘ë‹µ ì›ë³¸]")
        print(gpt_response_content)
        return gpt_response_content
    except openai.error.OpenAIError as e:
        print(f"DEBUG: ì˜¤ë¥˜: OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return ""
    except Exception as e:
        print(f"DEBUG: ì˜¤ë¥˜: GPT í˜¸ì¶œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")
        return ""

# --- âœ… GPT ì‘ë‹µ íŒŒì‹± í•¨ìˆ˜ ---
def extract_json_from_gpt_response(gpt_response_content: str) -> str:
    """GPT ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ JSON ë°°ì—´ ë˜ëŠ” ê°ì²´ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    match = re.search(r"\[.*\]|{.*}", gpt_response_content, re.DOTALL)
    return match.group(0) if match else "[]"


def safe_parse_json(response_text: str):
    """GPT ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ JSONì„ ì•ˆì „í•˜ê²Œ íŒŒì‹±í•©ë‹ˆë‹¤."""
    try:
        json_str = extract_json_from_gpt_response(response_text)
        return json.loads(json_str) if json_str.strip() else []
    except (json.JSONDecodeError, Exception) as e:
        print(f"ì˜¤ë¥˜: JSON íŒŒì‹± ì‹¤íŒ¨: {e} - ì‘ë‹µ ë‚´ìš©: '{response_text[:200]}...'")
        return []


# --- âœ… í—¬í¼: Scholarship ê°ì²´ë¥¼ GPT-friendly dictë¡œ ë³€í™˜ ---
def _scholarship_to_simplified_dict(scholarship_obj):
    """GPTê°€ ìƒì„¸ ë¹„êµë¥¼ í•  ìˆ˜ ìˆë„ë¡ í•„ìš”í•œ í•„ë“œë§Œ ì „ë‹¬í•©ë‹ˆë‹¤."""
    return {
        "product_id": scholarship_obj.product_id,
        "name": scholarship_obj.name,
        "product_type": scholarship_obj.product_type,
        "university_type": scholarship_obj.university_type,
        "academic_year_type": scholarship_obj.academic_year_type,
        "major_field": scholarship_obj.major_field,
        "region": scholarship_obj.region,
        "grade_criteria_details": scholarship_obj.grade_criteria_details,
        "income_criteria_details": scholarship_obj.income_criteria_details,
        "specific_qualification_details": scholarship_obj.specific_qualification_details,
    }


# --- âœ… ê¸°ë³¸ í•„í„°ë§ í•¨ìˆ˜ë“¤ ---
def filter_scholarships_by_date(scholarships_queryset: QuerySet) -> QuerySet:
    current_date = datetime.now().date()
    filtered_qs = scholarships_queryset.filter(
        recruitment_start_date__lte=current_date,
        recruitment_end_date__gte=current_date,
    )
    print(f"DEBUG: [0. ë‚ ì§œ í•„í„°ë§] í•„í„°ë§ í›„ ì¥í•™ê¸ˆ ìˆ˜: {filtered_qs.count()}")
    return filtered_qs


def filter_basic(scholarships_queryset: QuerySet, user_profile: UserScholarship) -> QuerySet:
    print(f"DEBUG: [1. ê¸°ë³¸ í•„í„°ë§] ì‚¬ìš©ì í”„ë¡œí•„: ëŒ€í•™='{user_profile.university_type}', í•™ë…„='{user_profile.academic_year_type}', ì „ê³µ='{user_profile.major_field}'")
    current_filtered_qs = scholarships_queryset

    # ëŒ€í•™ ìœ í˜• í•„í„°ë§
    if user_profile.university_type and user_profile.university_type.strip():
        all_university_types = current_filtered_qs.values_list('university_type', flat=True).distinct()
        user_univ_type_normalized = user_profile.university_type.strip().replace('-', '~')
        matching_types = [db_type for db_type in all_university_types if user_univ_type_normalized in db_type.replace('-', '~')]
        if matching_types:
            current_filtered_qs = current_filtered_qs.filter(university_type__in=matching_types)

    # í•™ë…„ ìœ í˜• í•„í„°ë§
    if user_profile.academic_year_type and user_profile.academic_year_type.strip():
        all_academic_years_in_db = current_filtered_qs.values_list('academic_year_type', flat=True).distinct()
        user_academic_year_normalized = user_profile.academic_year_type.strip().replace(' ', '')
        matching_academic_years = [db_year for db_year in all_academic_years_in_db if user_academic_year_normalized in db_year.replace(' ', '')]
        if matching_academic_years:
            current_filtered_qs = current_filtered_qs.filter(academic_year_type__in=matching_academic_years)

    # ì „ê³µ í•„í„°ë§
    user_major_field = getattr(user_profile, 'major_field', None)
    if user_major_field and user_major_field.strip():
        all_major_keywords = ["í•´ë‹¹ì—†ìŒ", "ì œí•œì—†ìŒ", "ì „ê³µë¬´ê´€", "íŠ¹ì •í•™ê³¼"]
        q_objects = Q(major_field__icontains=user_major_field.strip()) | Q(major_field__in=all_major_keywords)
        current_filtered_qs = current_filtered_qs.filter(q_objects)

    print(f"DEBUG: [1. ê¸°ë³¸ í•„í„°ë§] ìµœì¢… ê¸°ë³¸ í•„í„°ë§ ì ìš© í›„ ì¥í•™ê¸ˆ ìˆ˜: {current_filtered_qs.count()}")
    return current_filtered_qs


def filter_by_region_preprocessed(scholarships_queryset: QuerySet, user_profile: UserScholarship) -> QuerySet:
    user_region_do = getattr(user_profile, 'region', '') or ""
    user_district = getattr(user_profile, 'district', '') or ""
    user_region_parts = list(filter(None, [user_region_do.strip(), user_district.strip()]))
    full_user_region = ' '.join(user_region_parts)
    print(f"DEBUG: [2. ì§€ì—­ í•„í„°ë§] ì¡°í•©ëœ ì‚¬ìš©ì ì§€ì—­: '{full_user_region}'")

    if not full_user_region:
        return scholarships_queryset.filter(region__icontains="ì „êµ­")

    exact_match_conditions = [full_user_region] + user_region_parts
    q_objects = Q(region__in=exact_match_conditions) | Q(region__icontains="ì „êµ­")
    filtered_qs = scholarships_queryset.filter(q_objects).distinct()
    print(f"DEBUG: [2. ì§€ì—­ í•„í„°ë§] í•„í„°ë§ í›„ ì¥í•™ê¸ˆ ìˆ˜: {filtered_qs.count()}")
    return filtered_qs


# --- âœ… GPT ê¸°ë°˜ ìµœì¢… ì¶”ì²œ í•¨ìˆ˜ ---
def recommend_final_scholarships_by_gpt(filtered_scholarships_queryset: QuerySet, user_profile: UserScholarship) -> List[Dict]:
    print(f"DEBUG: [3. GPT ìµœì¢… ì¶”ì²œ] GPT í˜¸ì¶œ ì „ í›„ë³´êµ° ìˆ˜: {filtered_scholarships_queryset.count()}")
    if filtered_scholarships_queryset.count() == 0:
        return []

    # --- 1ë‹¨ê³„: ì ìˆ˜ ê¸°ë°˜ ìƒ˜í”Œë§ ---
    user_region_do = getattr(user_profile, 'region', '') or ""
    user_district = getattr(user_profile, 'district', '') or ""
    full_user_region = ' '.join(filter(None, [user_region_do.strip(), user_district.strip()]))
    user_major = getattr(user_profile, 'major_field', '') or ""

    score_annotation = Case(
        When(region=full_user_region, then=Value(10)),
        When(region=user_region_do, then=Value(7)),
        When(major_field__icontains=user_major, then=Value(5)),
        When(region__icontains="ì „êµ­", then=Value(1)),
        default=Value(0),
        output_field=models.IntegerField(),
    )

    scored_queryset = filtered_scholarships_queryset.annotate(relevance_score=score_annotation).order_by('-relevance_score')
    sample_size = 20
    actual_sample_size = min(scored_queryset.count(), sample_size)
    sampled_queryset_for_gpt = scored_queryset[:actual_sample_size]
    print(f"DEBUG: [3. GPT ìµœì¢… ì¶”ì²œ] ì ìˆ˜ì œ ìƒ˜í”Œë§ í›„ GPT ë¶„ì„ ëŒ€ìƒ ìˆ˜: {len(sampled_queryset_for_gpt)}")

    sampled_scholarships_for_gpt = [_scholarship_to_simplified_dict(s) for s in sampled_queryset_for_gpt]

    # --- 2ë‹¨ê³„: GPT í”„ë¡¬í”„íŠ¸ ì‘ì„± ---
    user_info_dict = user_profile.to_dict()
    user_info_dict['region'] = full_user_region
    user_info_dict.pop('district', None)

    prompt = f"""ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ í”„ë¡œí•„ê³¼ ì¥í•™ê¸ˆ ìê²© ì¡°ê±´ì„ ë¹„êµí•˜ì—¬, ê°œì¸í™”ëœ ì¶”ì²œ ë©”ì‹œì§€ë¥¼ ì‘ì„±í•˜ëŠ” AI ì¹´í”¼ë¼ì´í„°ì…ë‹ˆë‹¤.

[ì‚¬ìš©ì í”„ë¡œí•„]
{json.dumps(user_info_dict, ensure_ascii=False, indent=2)}

[ë¶„ì„ ëŒ€ìƒ ì¥í•™ê¸ˆ ëª©ë¡]
{json.dumps(sampled_scholarships_for_gpt, ensure_ascii=False, indent=2)}

[ì—…ë¬´ ì§€ì‹œ]
ì‚¬ìš©ì í”„ë¡œí•„ê³¼ ì¥í•™ê¸ˆ ëª©ë¡ì„ ë¶„ì„í•˜ì—¬, ëª©ë¡ ë‚´ì— ìˆëŠ” **ì´ {actual_sample_size}ê°œì˜ ì¥í•™ê¸ˆ**ì„ ì í•©ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ JSON ë°°ì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

    **[ë§¤ìš° ì¤‘ìš”í•œ ê·œì¹™]**

    1.  **ì‚¬ì‹¤ ê¸°ë°˜ ì‘ì„±:** reason'ì„ ì‘ì„±í•  ë•ŒëŠ” ì•„ë˜ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¥´ê³ , **ê·œì¹™ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ** ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”. ì ˆëŒ€ ì¶”ì¸¡í•˜ê±°ë‚˜ ì—†ëŠ” ë‚´ìš©ì„ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
        ê·œì¹™1.  **ì§€ì—­ ì¡°ê±´:** ì‚¬ìš©ìì˜ ì§€ì—­('{user_info_dict.get("region")}')ê³¼ ì¥í•™ê¸ˆì˜ 'region'ì´ êµ¬ì²´ì ìœ¼ë¡œ ì¼ì¹˜í• ìˆ˜ë¡ ë†’ì€ ì ìˆ˜ë¥¼ ì£¼ì„¸ìš”. 'ì „êµ­'ì€ ê·¸ ë‹¤ìŒì…ë‹ˆë‹¤.
        ê·œì¹™2.  **ì„±ì  ì¡°ê±´:** ì‚¬ìš©ìì˜ ì„±ì (gpa_last_semester, gpa_overall)ê³¼ ì¥í•™ê¸ˆì˜ 'grade_criteria_details'ë¥¼ ë¹„êµí•˜ì—¬, ê¸°ì¤€ì„ ì¶©ì¡±í•˜ë©´ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”.
        ê·œì¹™3.  **ì†Œë“ ì¡°ê±´:** ì‚¬ìš©ìì˜ ì†Œë“ë¶„ìœ„('income_level')ì™€ ì¥í•™ê¸ˆì˜ 'income_criteria_details'ë¥¼ ë¹„êµí•˜ì—¬, ê¸°ì¤€ì— ë¶€í•©í•˜ë©´ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”.
        ê·œì¹™4.  **íŠ¹ì • ìê²© ì¡°ê±´ (ê°€ì‚°ì  í•­ëª©):**
            - ë§Œì•½ ì‚¬ìš©ìì˜ 'is_multi_cultural_family'ê°€ Trueì´ê³ , ì¥í•™ê¸ˆ ì„¤ëª…(ì£¼ë¡œ 'specific_qualification_details')ì— 'ë‹¤ë¬¸í™”'ë¼ëŠ” í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë†’ì€ ê°€ì‚°ì ì„ ì£¼ì„¸ìš”.
            - ë§Œì•½ ì‚¬ìš©ìì˜ 'is_single_parent_family'ê°€ Trueì´ê³ , ì¥í•™ê¸ˆì˜ 'income_criteria_details'ì— 'í•œë¶€ëª¨', 'ê°€ì •í˜•í¸', 'ê²½ì œì‚¬ì •'ë¼ëŠ” í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë†’ì€ ê°€ì‚°ì ì„ ì£¼ì„¸ìš”.
            - ë§Œì•½ ì‚¬ìš©ìì˜ 'is_multiple_children_family'ê°€ Trueì´ê³ , ì¥í•™ê¸ˆì˜'income_criteria_details'ì— 'ë‹¤ìë…€'ë¼ëŠ” í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë†’ì€ ê°€ì‚°ì ì„ ì£¼ì„¸ìš”.
            - ë§Œì•½ ì‚¬ìš©ìì˜ 'is_national_merit'ê°€ Trueì´ê³ , ì¥í•™ê¸ˆì˜'income_criteria_details'ì— 'êµ­ê°€ìœ ê³µì' ë˜ëŠ” 'ë³´í›ˆ'ì´ë¼ëŠ” í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë†’ì€ ê°€ì‚°ì ì„ ì£¼ì„¸ìš”.
        ê·œì¹™5.  **ê¸°íƒ€ ì¡°ê±´:** ìœ„ ì¡°ê±´ ì™¸ì—ë„ ì‚¬ìš©ìì˜ ì „ê³µ, í•™ë…„ ë“±ì´ ì¥í•™ê¸ˆì˜ ì¡°ê±´ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì„¸ìš”.

    2.  **êµ¬ì²´ì ì¸ ì´ìœ  ì œì‹œ:** 'reason'ì—ëŠ” ì™œ ì´ ì¥í•™ê¸ˆì´ ì‚¬ìš©ìì—ê²Œ ì í•©í•œì§€, ì–´ë–¤ ì¡°ê±´(ì§€ì—­, ì„±ì , ì†Œë“, ì „ê³µ, í•™ë…„, íŠ¹ì • ìê²© ë“±)ì´ ì–´ë–»ê²Œ ë¶€í•©í•˜ëŠ”ì§€ë¥¼ **3ë¬¸ì¥ ì´ìƒ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ í˜•íƒœë¡œ** ì‘ì„±í•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, â€œê·€í•˜ëŠ” ê²½ê¸° ì§€ì—­ì˜ 4í•™ë…„ìƒìœ¼ë¡œ, í•´ë‹¹ ì¥í•™ê¸ˆì˜ ì§€ì—­ ìš”ê±´ê³¼ í•™ë…„ ì¡°ê±´ì„ ëª¨ë‘ ì¶©ì¡±í•©ë‹ˆë‹¤. ë˜í•œ ì„±ì  ê¸°ì¤€(3.5 ì´ìƒ)ì„ ë§Œì¡±í•˜ë©°, ì „êµ­ ë‹¨ìœ„ë¡œ ì§€ì› ê°€ëŠ¥í•´ ì ‘ê·¼ì„±ì´ ë†’ìŠµë‹ˆë‹¤.â€ ì™€ ê°™ì€ í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš”

    **[ì¶œë ¥ í˜•ì‹]**
    - ê° í•­ëª©ì€ 'product_id'ì™€ 'reason' ë‘ ê°œì˜ í‚¤ë¥¼ ê°€ì§„ JSON ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤.
    - 'reason'ì€ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ìµœì¢… ì¶”ì²œ ì‚¬ìœ (í•œêµ­ì–´ ë¬¸ìì—´)ì…ë‹ˆë‹¤. ë§Œì•½ ê·œì¹™4ë¡œ ì¸í•´ ê°€ì‚°ì ì„ ì–»ì€ ê²½ìš°, 'reason'ì— ê·¸ì™€ ê´€ë ¨ëœ ë‚´ìš©ì„ ë°˜ë“œì‹œ ì„œìˆ í•˜ì„¸ìš”.
    - 'product_id'ëŠ” ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”.

    **[ì¶œë ¥ ì˜ˆì‹œ]**
    [
      {{
        "product_id": "ì¥í•™ê¸ˆB_ì§€ìì²´B",
        "reason": "ê±°ì£¼í•˜ì‹œëŠ” 'ê²½ê¸°ë„ íŒŒì£¼ì‹œ' ì§€ì—­ ì¡°ê±´ì— ë¶€í•©í•˜ë©°, ì§ì „ í•™ê¸° ì„±ì (4.1)ì´ ìš”êµ¬ ê¸°ì¤€(3.5 ì´ìƒ)ì„ ì¶©ì¡±í•©ë‹ˆë‹¤."
      }},

      {{
        "product_id": "ì¥í•™ê¸ˆA_ì¬ë‹¨A",
        "reason": "'ë‹¤ìë…€ ê°€ì •' ìê²©ì— í•´ë‹¹í•˜ë©°, 'ì „êµ­' ë‹¨ìœ„ë¡œ ì§€ì› ê°€ëŠ¥í•˜ì—¬ ì§€ì—­ ì œí•œì´ ì—†ìŠµë‹ˆë‹¤."
      }}
    ]
    """

    # --- 3ë‹¨ê³„: GPT í˜¸ì¶œ ---
    gpt_response_content = call_gpt(prompt)
    parsed_response = safe_parse_json(gpt_response_content)

    if not isinstance(parsed_response, list) or not parsed_response:
        print("DEBUG: GPT í˜¸ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ì‘ë‹µ ë¹„ì •ìƒ â†’ í´ë°± ì‹¤í–‰")
        fallback_qs = scored_queryset[:min(scored_queryset.count(), 20)]
        return [
            {
                "product_id": s.product_id,
                "reason": f"ì¡°ê±´ ì¼ì¹˜ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìë™ ì¶”ì²œëœ '{s.name}' ì¥í•™ê¸ˆì…ë‹ˆë‹¤.",
                "scholarship": s,
            }
            for s in fallback_qs
        ]

    # --- 4ë‹¨ê³„: GPT ì‘ë‹µ ê²€ì¦ ---
    valid_recommendations = []
    sampled_ids_map = {s.product_id: s for s in sampled_queryset_for_gpt}

    print("\n" + "=" * 25 + " GPT ì‘ë‹µ ìµœì†Œ ê²€ì¦ ì‹œì‘ " + "=" * 25)
    for item in parsed_response:
        product_id = item.get('product_id')
        if isinstance(item, dict) and product_id and product_id in sampled_ids_map:
            item['scholarship'] = sampled_ids_map[product_id]
            valid_recommendations.append(item)
            print(f"  âœ… ìœ íš¨: {product_id}")
        else:
            print(f"  âŒ ë¬´íš¨: {product_id}")
    print("=" * 25 + " ê²€ì¦ ì™„ë£Œ " + "=" * 25 + "\n")

    if not valid_recommendations:
        print("ê²½ê³ : ê²€ì¦ì„ í†µê³¼í•œ ì¶”ì²œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. â†’ ì ìˆ˜ ê¸°ë°˜ í´ë°± ì‹¤í–‰")
        fallback_qs = scored_queryset[:min(scored_queryset.count(), 20)]
        return [
            {
                "product_id": s.product_id,
                "reason": f"ì¡°ê±´ ì¼ì¹˜ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìë™ ì¶”ì²œëœ '{s.name}' ì¥í•™ê¸ˆì…ë‹ˆë‹¤.",
                "scholarship": s,
            }
            for s in fallback_qs
        ]

    final_results = valid_recommendations[:20]
    print(f"DEBUG: [4. GPT ìµœì¢… ì¶”ì²œ] ìµœì¢… ë°˜í™˜ ìˆ˜: {len(final_results)}")
    return final_results


# --- âœ… ì´ê´„ ì‹¤í–‰ í•¨ìˆ˜ ---
def recommend(user_id: int) -> List[Dict]:
    """ì£¼ì–´ì§„ ì‚¬ìš©ì IDì— ëŒ€í•´ ì¥í•™ê¸ˆ ì¶”ì²œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    print(f"DEBUG: [ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹œì‘] ì‚¬ìš©ì ID: {user_id}")
    try:
        user_profile = UserScholarship.objects.get(user_id=user_id)
    except UserScholarship.DoesNotExist:
        print(f"ì˜¤ë¥˜: ì‚¬ìš©ì ID {user_id}ì— í•´ë‹¹í•˜ëŠ” í”„ë¡œí•„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    scholarships = Scholarship.objects.all()
    scholarships = filter_basic(scholarships, user_profile)
    scholarships = filter_by_region_preprocessed(scholarships, user_profile)

    final_recommendations = recommend_final_scholarships_by_gpt(scholarships, user_profile)
    print(f"DEBUG: [ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ] ìµœì¢… ì¶”ì²œ ì¥í•™ê¸ˆ ìˆ˜: {len(final_recommendations)}")

    return [
        {"product_id": r["product_id"], "reason": r["reason"]}
        for r in final_recommendations
    ]